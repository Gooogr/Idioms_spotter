{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import Value, ClassLabel, Features, Sequence\n",
    "from collections import defaultdict\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook convert raw JSONs of PIE and MAGPIE dataset to HuggingFace Dataset format for NER task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PIE = '../data/raw/PIE_annotations_all_v2.json'\n",
    "PATH_MAGPIE = '../data/raw/magpie-corpus.jsonl'\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_df = pd.read_json(PATH_PIE)\n",
    "magpie_df = pd.read_json(PATH_MAGPIE, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magpie_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare PIE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str, offsets: list[list[int]]=None) -> tuple[list[str], list[str]]:\n",
    "    '''\n",
    "    Apply word tokenization for input text and marked NER tokens. Each token in the source \n",
    "    line is assumed to be separated by a space. Code uses IOB format.\n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        offsets (list[list[int]]): comprehended list with start:end indecies of each NER in text\n",
    "    Returns:\n",
    "        word_tokens (list[str]): list of word tokens\n",
    "        pie_tokens (list[str]): list of coresponded NER labels\n",
    "\n",
    "    Example:\n",
    "    text: 'The deal was negotiated behind closed doors .'    \n",
    "    offsets: [[24, 30], [31, 37], [38, 44]]\n",
    "    \n",
    "    Example idiom is 'behind closed doors' and function output will be\n",
    "    ['The', 'deal', 'was', 'negotiated', 'behind', 'closed', 'doors', '.']\n",
    "    ['O', 'O', 'O', 'O', 'B-PIE', 'I-PIE', 'I-PIE', 'O']\n",
    "    '''\n",
    "    word_tokens = []\n",
    "    pie_tokens = []\n",
    "    start = 0\n",
    "\n",
    "    is_first_pie_token = True\n",
    "\n",
    "    if not offsets:\n",
    "        word_tokens = text.split()\n",
    "        pie_tokens = ['O'] * len(word_tokens)\n",
    "        return word_tokens, pie_tokens\n",
    "\n",
    "    for offset in offsets:\n",
    "        offset_start = offset[0]\n",
    "        offset_end = offset[1]\n",
    "\n",
    "        # Add tokens before current offset\n",
    "        substr = text[start:offset_start]\n",
    "        substr_tokens = substr.split()\n",
    "        word_tokens.extend(substr_tokens)\n",
    "        pie_tokens.extend(['O'] * len(substr_tokens))\n",
    "\n",
    "        # Add offset tokens\n",
    "        substr = text[offset_start:offset_end]\n",
    "        substr_tokens = substr.split()\n",
    "        word_tokens.extend(substr_tokens)\n",
    "\n",
    "        sbstr_pie_tokens = ['I-PIE'] * len(substr_tokens)\n",
    "        if is_first_pie_token:\n",
    "            sbstr_pie_tokens[0] = 'B-PIE'\n",
    "            is_first_pie_token = False\n",
    "        pie_tokens.extend(sbstr_pie_tokens)\n",
    "        start = offset_end\n",
    "\n",
    "    # Add the substring after the last offset\n",
    "    substr = text[start:]\n",
    "    substr_tokens = substr.split()\n",
    "    word_tokens.extend(substr_tokens)\n",
    "    pie_tokens.extend(['O'] * len(substr_tokens))\n",
    "\n",
    "    return word_tokens, pie_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove PIEs offsets from controversial objects. From docs:<br>\n",
    "`PIE_label: label indicating whether this sentence contains the PIE in question ('y') or not ('n')`\n",
    "\n",
    "For example 'They do this even though they may break the conventions from time to time .'\n",
    "doesn't contain 'break even' idiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_df['PIE_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_df.loc[pie_df['PIE_label'] == 'n', 'offsets'] = None "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract NERs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only 3-rd sentence with NER inside\n",
    "pie_df['context_pie_only'] = pd.DataFrame(pie_df['context'].to_list())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_df[['tokens', 'ner_tags']] = pie_df.apply(lambda x: tokenize_text(x.context_pie_only, x.offsets), \n",
    "             axis=1, result_type='expand')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only relevant features for final view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_df['is_pie'] = pie_df['PIE_label'] == 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_df = pie_df[['idiom', 'is_pie', 'tokens', 'ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MAGPIE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magpie_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "plt.title('MAGPIE labels confidence distribution')\n",
    "sns.histplot(magpie_df['confidence'], edgecolor='black', bins=20)\n",
    "plt.xlabel('Annotation confidence level')\n",
    "plt.ylabel('Objects amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analogy of the author of the corpus, we will use a threshold value of 75% confidence. \n",
    "In analogy with the PIE corpus, all examples with a value below the threshold will receive a False label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magpie_df['is_pie'] = magpie_df['confidence'] > 0.75\n",
    "magpie_df.loc[magpie_df['is_pie'] == False, 'offsets'] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magpie_df['context_pie_only'] = pd.DataFrame(magpie_df['context'].to_list())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magpie_df[['tokens', 'ner_tags']] = magpie_df.apply(lambda x: tokenize_text(x.context_pie_only, x.offsets), \n",
    "             axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magpie_df = magpie_df[['idiom', 'is_pie', 'tokens', 'ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magpie_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DataFrames to HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pie_df, magpie_df], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupl_amount = df['tokens'].str.join(sep='').duplicated().sum()\n",
    "print(f'Duplicates amount:{dupl_amount}')\n",
    "print(f'Duplicates ratio {dupl_amount/len(df) :.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[~df['tokens'].str.join(sep='').duplicated(), :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "plt.title('True vs False PIEs amount')\n",
    "df['is_pie'].value_counts().plot(kind='bar', ax=ax)\n",
    "ax.bar_label(ax.containers[-1], label_type='edge')\n",
    "plt.ylim([None, 60000])\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid_test = train_test_split(df, test_size=0.2, \n",
    "                                     stratify = df['is_pie'],\n",
    "                                     shuffle=True, \n",
    "                                     random_state=SEED)\n",
    "\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, \n",
    "                                     stratify = valid_test['is_pie'],\n",
    "                                     shuffle=True, \n",
    "                                     random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['is_pie'].value_counts())\n",
    "print(valid['is_pie'].value_counts())\n",
    "print(test['is_pie'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define feature schema and create DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\"idiom\": Value(\"string\"), \n",
    "                     \"is_pie\": Value(\"bool\"),\n",
    "                     \"tokens\": Sequence(Value('string')),\n",
    "                     \"ner_tags\": Sequence(ClassLabel(num_classes=3, names=['O', 'B-PIE', 'I-PIE'])) })\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict()\n",
    "dataset_dict['train'] = Dataset.from_pandas(train, features=features, preserve_index=False)\n",
    "dataset_dict['validation'] = Dataset.from_pandas(valid, features=features, preserve_index=False)\n",
    "dataset_dict['test'] = Dataset.from_pandas(test, features=features, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['train'].features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save locally and push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict.save_to_disk('../data/processed/pie_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login()\n",
    "# dataset_dict.push_to_hub(\"Gooogr/pie_idioms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idioms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c73e6cda88f45cd3262a3bc02cf7e06d8f47d9d1bab5bcac89f1bfbb1a6ba341"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
